{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re,gc,random\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\n\n\nclass CONFIG:\n    MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n    TRAIN_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/train.csv\"\n    SEED = 42 \n    MODEL = \"NLLB-DISTIL-600M\"\n    EPOCHS = 15\n    MAX_LEN = 128\n    OUTPUT = '/kaggle/working/'\n    BATCH_SIZE = 8\n    GRAD_ACCUM = 2\n    SRC_LANG = \"arb_Arab\"\n    TRGET_LANG = \"eng_Latn\"\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    GPU = torch.cuda.get_device_name(0)\n    \n    def create_display(self) -> None:\n        \n        print(f\"{'='*70}\")\n        print(\"TRAINING PIPELINE RUNNING....\")\n        print(f\"{'='*70}\")\n        print(f\"DEVICE: {self.DEVICE}\")\n        print(f\"GPU:{self.GPU}\")\n        print(f\"MODEL:{self.MODEL}\")\n        print(f\"BATCH SIZE:{self.BATCH_SIZE}\")\n        print(f\"EPOCHS:{self.EPOCHS}\")\n        print(f\"{'='*70}\\n\")\n\n\nclass ALIGNER:\n    def __init__(self,csv_path:str):\n        self.csv_path = csv_path\n\n    def simple_sentence_aligner (self,csv_path):\n        df = pd.read_csv(csv_path)\n        aligned_data = []\n\n        for idx,row in df.iterrows():\n            src = str(row['transliteration'])\n            tgt = str(row['translation'])\n\n            tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n            src_lines = [s.strip() for s in src.split('\\n') if s.strip()]\n            if len(tgt_sents) > 1 and len(tgt_sents) == len(src_lines):\n                \n                for s, t in zip(src_lines, tgt_sents):\n                    \n                    if len(s) > 3 and len(t) > 3:\n                        aligned_data.append({'transliteration': s, 'translation': t})\n            else:\n                aligned_data.append({'transliteration': src, 'translation': tgt})\n\n    \n            \n        return pd.DataFrame(aligned_data)\n\n\nclass DS:\n    def __init__(self,df,tok,max_len):\n        self.s = df[\"src\"].tolist()\n        self.t = df[\"tgt\"].tolist()\n        self.tok = tok\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.s)\n\n    def __getitem__(self, i):\n        a = self.tok(self.s[i], max_length=self.max_len, truncation=True)\n        b = self.tok(text_target=self.t[i], max_length=self.max_len, truncation=True)\n        return {\"input_ids\": a[\"input_ids\"], \"attention_mask\": a[\"attention_mask\"], \"labels\": b[\"input_ids\"]}\n\nclass NLLBTRAINER():\n    def __init__(self,model,tokenizer,val_ds,train_ds,batch_size,grad_accum,epoches,output_dir):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.val_ds = val_ds\n        self.train_ds = train_ds\n        self.batch_size = batch_size\n        self.grad_accum = grad_accum\n        self.epoches = epoches\n        self.output_dir = output_dir\n\n    def train(self):\n        collator = DataCollatorForSeq2Seq(\n            self.tokenizer,\n            model=self.model, \n            padding=True)\n\n        args = Seq2SeqTrainingArguments(\n            \n            output_dir = self.output_dir,\n            eval_strategy = \"epoch\",\n            save_strategy=\"no\",\n            learning_rate = 3e-4,\n            per_device_train_batch_size=self.batch_size,\n            per_device_eval_batch_size=self.batch_size,\n            gradient_accumulation_steps=self.grad_accum,\n            num_train_epochs = self.epoches,\n            warmup_ratio = 0.1,\n            fp16 = True,\n            logging_steps = 50,\n            report_to=\"none\",\n            remove_unused_columns=False\n           \n        )\n             \n\n        trainer = Seq2SeqTrainer(\n            \n             \n            model=self.model,\n            args=args,\n            train_dataset=self.train_ds,\n            eval_dataset=self.val_ds,\n            tokenizer=self.tokenizer,\n            data_collator=collator)\n        \n        trainer.train()\n        self.model.save_pretrained(self.output_dir)\n        self.tokenizer.save_pretrained(self.output_dir)\n        \n    \n\ndef clean(t):\n    if pd.isna(t): return \"\"\n    return re.sub(r\"\\s+\", \" \", str(t)).strip()\n\ndef main ():\n    config = CONFIG()\n    config.create_display()\n\n    alliner = ALIGNER(config.TRAIN_PATH)\n    train_df = alliner.simple_sentence_aligner(config.TRAIN_PATH)\n    train_df[\"src\"] = train_df[\"transliteration\"].apply(clean)\n    train_df[\"tgt\"] = train_df[\"translation\"].apply(clean)\n    train_df = train_df[train_df[\"src\"].str.len() > 10]\n    train_df = train_df.sample(frac=1, random_state=config.SEED).reset_index(drop=True)\n\n    val_df = train_df.iloc[:150]\n    train_df = train_df.iloc[150:]\n\n    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, src_lang=config.SRC_LANG, tgt_lang=config.TRGET_LANG)\n    model = AutoModelForSeq2SeqLM.from_pretrained(config.MODEL_NAME)\n    ENG_TOKEN_ID = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n    model.to(config.DEVICE)\n    train_ds = DS(train_df,tokenizer,config.MAX_LEN)\n    val_ds = DS(val_df,tokenizer,config.MAX_LEN)\n\n\n    trainer=NLLBTRAINER(model,tokenizer,val_ds,train_ds,config.BATCH_SIZE,config.GRAD_ACCUM,config.EPOCHS,config.OUTPUT)\n    trainer.train()\n    \n    print(f\"\\n{'='*70}\")\n    print(\" All training completed!\")\n    print(f\"{'='*70}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T09:57:09.027506Z","iopub.execute_input":"2025-12-24T09:57:09.027721Z","iopub.status.idle":"2025-12-24T10:29:18.022880Z","shell.execute_reply.started":"2025-12-24T09:57:09.027699Z","shell.execute_reply":"2025-12-24T10:29:18.021620Z"}},"outputs":[{"name":"stderr","text":"2025-12-24 09:57:22.816087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766570242.987836      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766570243.043525      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766570243.449737      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766570243.449774      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766570243.449777      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766570243.449780      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"======================================================================\nTRAINING PIPELINE RUNNING....\n======================================================================\nDEVICE: cuda\nGPU:Tesla P100-PCIE-16GB\nMODEL:NLLB-DISTIL-600M\nBATCH SIZE:8\nEPOCHS:15\n======================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0145b4f9f992473da073e68d7f1a4219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d3fbf3e8ee4ea9bb13f428871320e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1923a1f49afc40af9060b94e3cc23115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"513e3b17b5574c23ae06afdbc753a050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24de3c294c35486aae1fffe5abcac110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a15d1d065d04cd9ace421835a17c674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b769e9a8dba84af287b8de425add3302"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45395075922e47b3bc2ec0cbf8f0fe08"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1252809526.py:118: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1335' max='1335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1335/1335 31:14, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.780700</td>\n      <td>2.156007</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.883000</td>\n      <td>1.767273</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.425200</td>\n      <td>1.639188</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.159700</td>\n      <td>1.596382</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.927400</td>\n      <td>1.627087</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.741600</td>\n      <td>1.720291</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.608100</td>\n      <td>1.794332</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.488600</td>\n      <td>1.919731</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.399400</td>\n      <td>1.990794</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.300000</td>\n      <td>2.052968</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.244700</td>\n      <td>2.131294</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.197400</td>\n      <td>2.179425</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.162400</td>\n      <td>2.221823</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.145000</td>\n      <td>2.243516</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.128700</td>\n      <td>2.249513</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n All training completed!\n======================================================================\n\n","output_type":"stream"}],"execution_count":1}]}